# -*- coding: utf-8 -*-
"""Assignment 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z7_G0KGu9dh7od3W6oTKbT-TZ1Gvpb4f

# CAMPUS RECURITMENT PREDICTION ASSIGNMENT
### Amanjot Kaur Sohal
### C0916838
"""

from google.colab import drive
drive.mount('/content/drive')

# Importing the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# reading the CSV file from Google Drive
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv')

# displaying the dataset
df.head()

df.info()

df.describe()

"""# Section 1: Data Preprocessing

# 1.1 Handle missing values
"""

print("Missing values:\n", df.isnull().sum())

"""# Handling missing salary values"""

# - Setin salary to 0 if the student is 'Not Placed'
# - Imputing salary with the mean of 'Placed' students' salaries if the status is 'Placed'
df['salary'] = df.apply(lambda row: 0 if row['status'] == 'Not Placed' else row['salary'], axis=1)
df['salary'].fillna(df[df['status'] == 'Placed']['salary'].mean(), inplace=True)

# Confirming there are no missing values in 'salary' anymore
print("Missing values after imputation:\n", df.isnull().sum())

"""# 1.2 Encoding categorical variables"""

# Encoding 'gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation', 'status'
le = LabelEncoder()

df['gender'] = le.fit_transform(df['gender'])
df['ssc_b'] = le.fit_transform(df['ssc_b'])
df['hsc_b'] = le.fit_transform(df['hsc_b'])
df['hsc_s'] = le.fit_transform(df['hsc_s'])
df['degree_t'] = le.fit_transform(df['degree_t'])
df['workex'] = le.fit_transform(df['workex'])
df['specialisation'] = le.fit_transform(df['specialisation'])
df['status'] = le.fit_transform(df['status'])

"""# 1.3 Detecting Outliers"""

# Detecting outliers using IQR method for 'salary'
Q1 = df['salary'].quantile(0.25)
Q3 = df['salary'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df = df[(df['salary'] >= lower_bound) & (df['salary'] <= upper_bound)]

"""# 1.4 Checking Duplicates"""

duplicates = df[df.duplicated()]

# Display the number of duplicates
print(f"Number of duplicate rows: {duplicates.shape[0]}")

# @title 1.5 Checking Class Imbalance

class_distribution = df['status'].value_counts()
print(class_distribution)

# Import necessary libraries
from imblearn.over_sampling import SMOTE

# Define features (X) and target variable (y)
X = df.drop(columns=['status'])
y = df['status']

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to balance the classes
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create a new DataFrame with resampled data
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['status'] = y_resampled

# Check the class distribution after resampling
class_distribution = df_resampled['status'].value_counts()
print("Class distribution after SMOTE:\n", class_distribution)

"""# Section 2: Feature Extraction

# 2.1 Creating new feature "average_academic_score" by averaging SSC, HSC, and Degree percentages
"""

df['average_academic_score'] = (df['ssc_p'] + df['hsc_p'] + df['degree_p']) / 3

"""# 2.2 Creating another feature for total experience score"""

df['work_experience_score'] = np.where(df['workex'] == 1, df['etest_p'] * 1.2, df['etest_p'])

"""# Section 3: Data Visualization

# 3.1 Distribution of SSC percentage based on placement status
"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='ssc_p', hue='status', multiple="stack", kde=True)
plt.title('Distribution of SSC Percentage by Placement Status')
plt.show()

"""# 3.2 Correlation Heatmap"""

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Features')
plt.show()

"""# 3.3 Boxplot of Degree Percentage by Placement Status"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='status', y='degree_p')
plt.title('Degree Percentage by Placement Status')
plt.show()

"""# 3.4 Countplot of Specialisation vs Placement Status"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='specialisation', hue='status')
plt.title('Specialisation vs Placement Status')
plt.show()

"""# Section 4: Feature Selection

# 4.1 Correlation Analysis
"""

# dropping the highly-correlated features
df = df.drop(columns=['ssc_p', 'hsc_p', 'degree_p', 'etest_p'])

"""# 4.2 Model-based feature selection using Random Forest importance"""

rf = RandomForestClassifier(random_state=42)
X = df.drop(['status'], axis=1)  # Features
y = df['status']  # Target

# Splitting into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

rf.fit(X_train, y_train)
feature_importances = pd.Series(rf.feature_importances_, index=X.columns)

# Plotting Feature Importances
plt.figure(figsize=(10, 6))
feature_importances.nlargest(10).plot(kind='barh')
plt.title('Top 10 Feature Importances from RandomForest')
plt.show()

# dropping the less contributing features
df = df.drop(columns=['ssc_b', 'hsc_s', 'degree_t'])

"""# Section 5: Model Training and Evaluation"""

# Import necessary libraries for models and evaluation
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

"""# 5.1 Splitting the Data into Training and Testing Sets"""

# Splitting the dataset into training and testing sets (70% training, 30% testing)
X = df.drop(['status'], axis=1)  # Features
y = df['status']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print("Training set shape: ", X_train.shape)
print("Testing set shape: ", X_test.shape)

X_train.head()

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

# Assuming 'numerical_features' is a list of the numerical columns
numerical_features = ['mba_p', 'salary', 'average_academic_score', 'work_experience_score']

# Define the ColumnTransformer
ct = ColumnTransformer([
    ('scale', StandardScaler(), numerical_features)
], remainder='passthrough')

# Fit and transform on training data, and transform on test data
X_train = ct.fit_transform(X_train)
X_test = ct.transform(X_test)

"""# 5.2 Model Selection: Training Multiple Models

# Random Forest Classifier
"""

# Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Prediction using Random Forest
y_pred_rf = rf_model.predict(X_test)

"""# Support Vector Machine (SVM)"""

# Support Vector Classifier (SVM)
svm_model = SVC(probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# Prediction using SVM
y_pred_svm = svm_model.predict(X_test)

"""# Logistic Regression"""

# Logistic Regression
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)

# Prediction using Logistic Regression
y_pred_lr = lr_model.predict(X_test)

"""# 5.3 Voting Classifier"""

# Voting Classifier (Soft Voting)
voting_clf = VotingClassifier(estimators=[('rf', rf_model), ('svm', svm_model), ('lr', lr_model)], voting='soft')
voting_clf.fit(X_train, y_train)

# Prediction using Voting Classifier
y_pred_voting = voting_clf.predict(X_test)

"""# 5.4 Model Evaluation: Metrics and Confusion Matrices"""

# Function to print evaluation metrics for each model
def evaluate_model(y_test, y_pred, model_name):
    print(f"--- {model_name} ---")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred, average='binary'))
    print("Recall:", recall_score(y_test, y_pred, average='binary'))
    print("F1 Score:", f1_score(y_test, y_pred, average='binary'))
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    print("\n-------------------------\n")

# Evaluate Random Forest
evaluate_model(y_test, y_pred_rf, "Random Forest")

# Evaluate SVM
evaluate_model(y_test, y_pred_svm, "SVM")

# Evaluate Logistic Regression
evaluate_model(y_test, y_pred_lr, "Logistic Regression")

# Evaluate Voting Classifier
evaluate_model(y_test, y_pred_voting, "Voting Classifier")

"""# 5.5 Visualization of Results

# Confusion Matrix Visualization
"""

# Function to plot confusion matrices
def plot_confusion_matrix(y_test, y_pred, model_name):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix for {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Plot confusion matrix for each model
plot_confusion_matrix(y_test, y_pred_rf, "Random Forest")
plot_confusion_matrix(y_test, y_pred_svm, "SVM")
plot_confusion_matrix(y_test, y_pred_lr, "Logistic Regression")
plot_confusion_matrix(y_test, y_pred_voting, "Voting Classifier")

"""# 5.6 Comparing Model Performance

# Bar Plot of Performance Metrics
"""

# Comparison of model performances
models = ['Random Forest', 'SVM', 'Logistic Regression', 'Voting Classifier']
accuracies = [
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_svm),
    accuracy_score(y_test, y_pred_lr),
    accuracy_score(y_test, y_pred_voting)
]

precisions = [
    precision_score(y_test, y_pred_rf, average='binary'),
    precision_score(y_test, y_pred_svm, average='binary'),
    precision_score(y_test, y_pred_lr, average='binary'),
    precision_score(y_test, y_pred_voting, average='binary')
]

recalls = [
    recall_score(y_test, y_pred_rf, average='binary'),
    recall_score(y_test, y_pred_svm, average='binary'),
    recall_score(y_test, y_pred_lr, average='binary'),
    recall_score(y_test, y_pred_voting, average='binary')
]

f1_scores = [
    f1_score(y_test, y_pred_rf, average='binary'),
    f1_score(y_test, y_pred_svm, average='binary'),
    f1_score(y_test, y_pred_lr, average='binary'),
    f1_score(y_test, y_pred_voting, average='binary')
]

# Plotting the metrics
fig, ax = plt.subplots(2, 2, figsize=(12, 10))

# Accuracy
ax[0, 0].bar(models, accuracies, color='lightblue')
ax[0, 0].set_title("Accuracy")

# Precision
ax[0, 1].bar(models, precisions, color='lightgreen')
ax[0, 1].set_title("Precision")

# Recall
ax[1, 0].bar(models, recalls, color='lightcoral')
ax[1, 0].set_title("Recall")

# F1 Score
ax[1, 1].bar(models, f1_scores, color='lightskyblue')
ax[1, 1].set_title("F1 Score")

plt.tight_layout()
plt.show()